{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/withpi/cookbook-withpi/blob/main/colabs/Preference_Collection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://withpi.ai\"><img src=\"https://withpi.ai/logoFullBlack.svg\" width=\"240\"></a>\n",
        "\n",
        "<a href=\"https://code.withpi.ai\"><font size=\"4\">Documentation</font></a>\n",
        "\n",
        "<a href=\"https://play.withpi.ai\"><font size=\"4\">Technique Catalog</font></a>"
      ],
      "metadata": {
        "id": "pi-masthead"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bwm4tjdnedp6"
      },
      "source": [
        "# WithPi Contract Calibration\n",
        "\n",
        "This colab assumes that you already went through [Input Generation](https://colab.research.google.com/github/withpi/cookbook-withpi/blob/main/colabs/Input_Generation.ipynb), and now wish to calibrate your prompt.\n",
        "\n",
        "We will walk through the same `Aesop AI` example, but you can load any contract here. Let's dig in!\n",
        "\n",
        "This should take about **15 minutes**, even if you're unfamiliar with Colab."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install and initialize SDK\n",
        "\n",
        "Connect to a regular CPU Python 3 runtime.  You won't need GPUs for this notebook.\n",
        "\n",
        "You'll need a WITHPI_API_KEY from https://play.withpi.ai.  Add it to your notebook secrets (the key symbol) on the left.\n",
        "\n",
        "Run the cell below to install packages and load the SDK"
      ],
      "metadata": {
        "id": "pi-setup-markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pi-setup"
      },
      "outputs": [],
      "execution_count": null,
      "source": [
        "%%capture\n",
        "\n",
        "%pip install withpi litellm\n",
        "\n",
        "import os\n",
        "from google.colab import userdata\n",
        "from litellm import completion\n",
        "from withpi import PiClient\n",
        "\n",
        "os.environ[\"WITHPI_API_KEY\"] = userdata.get('WITHPI_API_KEY')\n",
        "\n",
        "client = PiClient()\n",
        "\n",
        "def print_contract(contract):\n",
        "  for dimension in contract.dimensions:\n    print(dimension.label)\n",
        "    for sub_dimension in dimension.sub_dimensions:\n",
        "      print(f\"\\t{sub_dimension.description}\")\n",
        "\n",
        "def generate(system: str, user: str, model: str) -> str:\n",
        "  messages = [\n",
        "    {\n",
        "      \"content\": system,\n",
        "      \"role\": \"system\"\n",
        "    },\n",
        "    {\n",
        "      \"content\": prompt,\n",
        "      \"role\": \"user\"\n",
        "    }\n",
        "  ]\n",
        "  return completion(model=model,\n",
        "                    messages=messages).choices[0].message.content\n",
        "\n",
        "class printer(str):\n",
        "  def __repr__(self):\n",
        "    return self\n",
        "def prettyprint(response: str):\n",
        "  display(printer(response))\n",
        "\n",
        "def print_scores(pi_scores):\n",
        "  for dimension_name, dimension_scores in pi_scores.dimension_scores.items():\n",
        "    print(f\"{dimension_name}: {dimension_scores.total_score}\")\n",
        "    for subdimension_name, subdimension_score in dimension_scores.subdimension_scores.items():\n",
        "      print(f\"\\t{subdimension_name}: {subdimension_score}\")\n",
        "    print(\"\\n\")\n",
        "  print(\"---------------------\")\n",
        "  print(f\"Total score: {pi_scores.total_score}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oXJmb89i5iN5"
      },
      "outputs": [],
      "source": [
        "import httpx\n",
        "import pandas as pd\n",
        "from google.colab import data_table\n",
        "from withpi.types import Contract\n",
        "\n",
        "resp = httpx.get(\"https://raw.githubusercontent.com/withpi/cookbook-withpi/refs/heads/main/contracts/aesop_ai.json\")\n",
        "\n",
        "aesop_contract = Contract.model_validate_json(resp.content)\n",
        "\n",
        "for dimension in aesop_contract.dimensions:\n",
        "  print(dimension.label)\n",
        "  for sub_dimension in dimension.sub_dimensions:\n",
        "    print(f\"\\t{sub_dimension.description}\")\n",
        "\n",
        "df = pd.read_parquet(\"https://raw.githubusercontent.com/withpi/cookbook-withpi/refs/heads/main/datasets/aesop_ai_examples.parquet\")\n",
        "data_table.enable_dataframe_formatter()\n",
        "df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1FAoBqU7dwf"
      },
      "source": [
        "## Cluster Inputs\n",
        "\n",
        "We're going to label some inputs as \"good\" and \"bad\", but to do this it is helpful to focus on a few different types of input.  We'll use clustering to make sure we don't have to look at too many examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4NpCZhP6exgI"
      },
      "outputs": [],
      "source": [
        "input_topic_clusters = client.data.inputs.cluster(\n",
        "    inputs=[{\"identifier\": str(index), \"llm_input\": row[\"input\"]} for index, row in df.iterrows()],\n",
        ")\n",
        "\n",
        "df['cluster'] = ['']*len(df)\n",
        "for cluster in input_topic_clusters:\n",
        "  for identifier in cluster.inputs:\n",
        "    df.loc[int(identifier),'cluster'] = cluster.topic\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCHE6g_OCqCD"
      },
      "source": [
        "## Identify outliers\n",
        "\n",
        "Let's first score every input against the contract, adding that as a column.  Pi scoring is fast enough that serially processing the dataset is fine, though we could increase parallelism for more speed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57EujAhfC_qz"
      },
      "outputs": [],
      "source": [
        "df[\"uncalibrated_scores\"] = [client.contracts.score(contract=aesop_contract, llm_input=row[\"input\"], llm_output=row[\"output\"]).total_score for idx, row in df.iterrows()]\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMTmieU9xPgT"
      },
      "source": [
        "## Label data\n",
        "\n",
        "Now it's time to label examples against a simple statement.  **The response fully satisfies the input according to the contract**.  Valid responses are **Strongly Agree**, **Agree**, **Neutral**, **Disagree**, and **Strongly Disagree**, or simply **5** down to **1**.\n",
        "\n",
        "The below cell will select a high and low scoring exemplar from each cluster, asking you to respond **5** through **1**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kdaF7pnIGYxa"
      },
      "outputs": [],
      "source": [
        "df[\"label\"]= ['']*len(df)\n",
        "\n",
        "def get_label(row):\n",
        "  display(\"Input Prompt:\")\n",
        "  display(row.loc[\"input\"])\n",
        "  display(\"Output Response:\")\n",
        "  display(row.loc[\"output\"])\n",
        "  while True:\n",
        "    resp = input(\"Your rating from 1 to 5: \")\n",
        "    try:\n",
        "      if int(resp) not in [1,2,3,4,5]:\n",
        "        raise ValueError(\"Invalid\")\n",
        "    except:\n",
        "      display(\"Invalid input. Try again\")\n",
        "      continue\n",
        "    break\n",
        "  df.loc[row.name,'label'] = resp\n",
        "\n",
        "clusters = [x for _, x in df.groupby(df[\"cluster\"])]\n",
        "for cluster in clusters:\n",
        "  sorted = cluster.sort_values(by=['uncalibrated_scores'])\n",
        "  get_label(sorted.iloc[0])\n",
        "  get_label(sorted.iloc[-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yR-YSyrYM4-z"
      },
      "source": [
        "## Calibrate\n",
        "\n",
        "Now it's time to calibrate with the labelled sets.  The following cell will launch a job."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-4aZQ4YM9FL"
      },
      "outputs": [],
      "source": [
        "def to_rating(label):\n",
        "  match label:\n",
        "    case '1':\n",
        "      return \"Strongly Disagree\"\n",
        "    case '2':\n",
        "      return \"Disagree\"\n",
        "    case '3':\n",
        "      return \"Neutral\"\n",
        "    case '4':\n",
        "      return \"Agree\"\n",
        "    case '5':\n",
        "      return \"Strongly Agree\"\n",
        "\n",
        "labelled = df[df['label'] != '']\n",
        "contract_calibration_status = client.contracts.calibrate.start_job(\n",
        "    contract=aesop_contract,\n",
        "    examples=[{\"llm_input\": row['input'], \"llm_output\": row['output'], \"rating\": to_rating(row['label'])} for _, row in labelled.iterrows()]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfOpDKMsOffg"
      },
      "source": [
        "# Monitor for completion\n",
        "\n",
        "The next cell will monitor logs from the calibration job, ending when it's complete."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5jnONgywOg7o"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "while True:\n",
        "  calibrated_response = client.contracts.calibrate.retrieve(job_id=contract_calibration_status.job_id)\n",
        "  if (calibrated_response.state != 'QUEUED') and (calibrated_response.state != 'RUNNING'):\n",
        "    break\n",
        "\n",
        "  with client.contracts.calibrate.with_streaming_response.stream_messages(\n",
        "      job_id=contract_calibration_status.job_id, timeout=None) as response:\n",
        "    for line in response.iter_lines():\n",
        "          print(line)\n",
        "\n",
        "aesop_contract_calibrated = calibrated_response.calibrated_contract"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mn4kg54Yixzn"
      },
      "source": [
        "## Rescore after calibration\n",
        "\n",
        "Now add a new column with calibrated scores. You can examine these to see if they more closely align with the examples you labelled.  Ideally the score starts separating good responses from bad.\n",
        "\n",
        "If it does not, that suggests the properties you **really** care about aren't captured in your scoring dimensions and will need to be added.  Proceed to the playgrounds at http://play.withpi.ai to experiment with this.\n",
        "\n",
        "If this is looking good, you have a powerful function for improving your system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cxd-uOBaOQQh"
      },
      "outputs": [],
      "source": [
        "df[\"calibrated_scores\"] = [client.contracts.score(contract=aesop_contract_calibrated, llm_input=row[\"input\"], llm_output=row[\"output\"]).total_score for idx, row in df.iterrows()]\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAaeXLrijCrK"
      },
      "source": [
        "## Save calibrated contract\n",
        "\n",
        "The updated contract now has different weights assigned to its dimensions.  Save those for later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tpWpFWfEwo4j"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from google.colab import files\n",
        "\n",
        "filename = 'aesop_ai_calibrated.json'\n",
        "Path(filename).write_text(aesop_contract_calibrated.model_dump_json(indent=2))\n",
        "files.download(filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jT7s_nuJsHbM"
      },
      "source": [
        "## Next Steps\n",
        "\n",
        "Now that you have a calibrated contract, you can start incorporating Feedback to improve your system.   Proceed on to the [Feedback Clustering](https://colab.research.google.com/github/withpi/cookbook-withpi/blob/main/colabs/Feedback_Clustering.ipynb) colab to do this."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}