{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/withpi/cookbook-withpi/blob/main/colabs/Dataset_Import.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://withpi.ai\"><img src=\"https://withpi.ai/logoFullBlack.svg\" width=\"240\"></a>\n",
        "\n",
        "<a href=\"https://code.withpi.ai\"><font size=\"4\">Documentation</font></a>\n",
        "\n",
        "<a href=\"https://play.withpi.ai\"><font size=\"4\">Technique Catalog</font></a>"
      ],
      "metadata": {
        "id": "pi-masthead"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bwm4tjdnedp6"
      },
      "source": [
        "# WithPi Input Generation\n",
        "\n",
        "This colab assumes that you already went through [Contract Creation](https://colab.research.google.com/github/withpi/cookbook-withpi/blob/main/colabs/Contract_Creation.ipynb), and now you wish to build a small input set.\n",
        "\n",
        "We will walk through the same `Aesop AI` example, but you can load any contract here. Let's dig in!\n",
        "\n",
        "This should take about **15 minutes**, even if you're unfamiliar with Colab."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install and initialize SDK\n",
        "\n",
        "Connect to a regular CPU Python 3 runtime.  You won't need GPUs for this notebook.\n",
        "\n",
        "You'll need a WITHPI_API_KEY from https://play.withpi.ai.  Add it to your notebook secrets (the key symbol) on the left.\n",
        "\n",
        "Run the cell below to install packages and load the SDK"
      ],
      "metadata": {
        "id": "pi-setup-markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pi-setup"
      },
      "outputs": [],
      "execution_count": null,
      "source": [
        "%%capture\n",
        "\n",
        "%pip install withpi litellm\n",
        "\n",
        "import os\n",
        "from google.colab import userdata\n",
        "from litellm import completion\n",
        "from withpi import PiClient\n",
        "\n",
        "os.environ[\"WITHPI_API_KEY\"] = userdata.get('WITHPI_API_KEY')\n",
        "\n",
        "client = PiClient()\n",
        "\n",
        "def print_contract(contract):\n",
        "  for dimension in contract.dimensions:\n    print(dimension.label)\n",
        "    for sub_dimension in dimension.sub_dimensions:\n",
        "      print(f\"\\t{sub_dimension.description}\")\n",
        "\n",
        "def generate(system: str, user: str, model: str) -> str:\n",
        "  messages = [\n",
        "    {\n",
        "      \"content\": system,\n",
        "      \"role\": \"system\"\n",
        "    },\n",
        "    {\n",
        "      \"content\": prompt,\n",
        "      \"role\": \"user\"\n",
        "    }\n",
        "  ]\n",
        "  return completion(model=model,\n",
        "                    messages=messages).choices[0].message.content\n",
        "\n",
        "class printer(str):\n",
        "  def __repr__(self):\n",
        "    return self\n",
        "def print_response(response: str):\n",
        "  display(printer(response))\n",
        "\n",
        "def print_scores(pi_scores):\n",
        "  for dimension_name, dimension_scores in pi_scores.dimension_scores.items():\n",
        "    print(f\"{dimension_name}: {dimension_scores.total_score}\")\n",
        "    for subdimension_name, subdimension_score in dimension_scores.subdimension_scores.items():\n",
        "      print(f\"\\t{subdimension_name}: {subdimension_score}\")\n",
        "    print(\"\\n\")\n",
        "  print(\"---------------------\")\n",
        "  print(f\"Total score: {pi_scores.total_score}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7RRO3iXjYbY"
      },
      "source": [
        "# Load contract\n",
        "\n",
        "Load the `Aesop AI` example from Pi Labs cookbooks, or edit below to load a different one.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oXJmb89i5iN5"
      },
      "outputs": [],
      "source": [
        "import httpx\n",
        "from withpi.types import Contract\n",
        "\n",
        "resp = httpx.get(\"https://raw.githubusercontent.com/withpi/cookbook-withpi/refs/heads/main/contracts/aesop_ai.json\")\n",
        "\n",
        "aesop_contract = Contract.model_validate_json(resp.content)\n",
        "\n",
        "for dimension in aesop_contract.dimensions:\n",
        "  print(dimension.label)\n",
        "  for sub_dimension in dimension.sub_dimensions:\n",
        "    print(f\"\\t{sub_dimension.description}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1FAoBqU7dwf"
      },
      "source": [
        "## Generate an Input Set\n",
        "\n",
        "Given this structured description, let's build a Dataset containing a bunch of plausible moral lessons that could be used to exercise the contract.  This will take about 30 seconds to generate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4NpCZhP6exgI"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from google.colab import data_table\n",
        "\n",
        "data_generation_status = client.data.inputs.generate_seeds(\n",
        "    contract_description=aesop_contract.description,\n",
        "    num_inputs=10,\n",
        ")\n",
        "\n",
        "data_table.enable_dataframe_formatter()\n",
        "df = pd.DataFrame({\"input\": data_generation_status.data})\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSmwlrjSWtxl"
      },
      "source": [
        "# Augment with responses\n",
        "\n",
        "Now run inference with your favorite LLM to generate responses to the default prompt.  We will optimize this later.\n",
        "\n",
        "The below cell uses LiteLLM.  You can get a Gemini key from the left side for free to try it out.  You may need a small amount of money in your account to run to completion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ENQHR9XTW3LP"
      },
      "outputs": [],
      "source": [
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from tqdm.notebook import tqdm\n",
        "from litellm import completion\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"GEMINI_API_KEY\"] = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "def generate(prompt: str, pbar) -> str:\n",
        "  messages = [\n",
        "      {\n",
        "          \"content\": aesop_contract.description,\n",
        "          \"role\": \"system\"\n",
        "      },\n",
        "      {\n",
        "          \"content\": prompt,\n",
        "          \"role\": \"user\"\n",
        "      },\n",
        "  ]\n",
        "  result = completion(model=\"gemini/gemini-1.5-flash-8b-latest\",\n",
        "                      messages=messages).choices[0].message.content\n",
        "  pbar.update(1)\n",
        "  return result\n",
        "\n",
        "def do_inference():\n",
        "  futures = []\n",
        "  pbar = tqdm(total=len(df))\n",
        "  with ThreadPoolExecutor(max_workers=4) as executor:\n",
        "    for index, row in df.iterrows():\n",
        "      futures.append(executor.submit(generate, row[\"input\"], pbar))\n",
        "  return [future.result() for future in futures]\n",
        "\n",
        "df[\"output\"] = do_inference()\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMTmieU9xPgT"
      },
      "source": [
        "## Save the set\n",
        "\n",
        "We will come back to this in a future colab, so it's useful to capture.  Store it as a Parquet table, which you can download."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tpWpFWfEwo4j"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "filename = \"aesop_ai_examples.parquet\"\n",
        "df.to_parquet(filename)\n",
        "files.download(filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jT7s_nuJsHbM"
      },
      "source": [
        "## Next Steps\n",
        "\n",
        "Now you can try optimizing your prompt with the [Prompt Optimization](https://colab.research.google.com/github/withpi/cookbook-withpi/blob/main/colabs/Prompt_Optimization.ipynb) colab to put this to work."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}