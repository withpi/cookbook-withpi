{"cells": [{"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "view-in-github"}, "source": ["<a href=\"https://colab.research.google.com/github/withpi/cookbook-withpi/blob/main/colabs/Prompt_Optimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]}, {"cell_type": "markdown", "source": ["<a href=\"https://withpi.ai\"><img src=\"https://withpi.ai/logoFullBlack.svg\" width=\"240\"></a>\n", "\n", "<a href=\"https://code.withpi.ai\"><font size=\"4\">Documentation</font></a>\n", "\n", "<a href=\"https://play.withpi.ai\"><font size=\"4\">Technique Catalog</font></a>"], "metadata": {"id": "pi-masthead"}}, {"cell_type": "markdown", "metadata": {"id": "Bwm4tjdnedp6"}, "source": ["# Prompt Optimization Technique\n", "\n", "\n", "This colab assumes that you already went through [Input Generation](https://colab.research.google.com/github/withpi/cookbook-withpi/blob/main/colabs/Input_Generation.ipynb), and now wish to optimize your system prompt\n", "\n", "We will walk through the same `Aesop AI` example, but you can load any contract here. Let's dig in!\n", "\n", "This should take about **15 minutes**, even if you're unfamiliar with Colab."]}, {"cell_type": "markdown", "metadata": {"id": "9HBxNR2oerzC"}, "source": ["## Install and initialize SDK\n", "\n", "Connect to a regular CPU Python 3 runtime.  You won't need GPUs for this notebook.\n", "\n", "You'll need a WITHPI_API_KEY from https://play.withpi.ai.  Add it to your notebook secrets (the key symbol) on the left.\n", "\n", "Run the cell below to install packages and load the SDK"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "VXIRVg-sMv-S"}, "outputs": [], "source": ["%%capture\n", "\n", "%pip install withpi httpx pandas tqdm litellm jinja2\n", "\n", "import os\n", "from google.colab import userdata\n", "from withpi import PiClient\n", "\n", "os.environ[\"WITHPI_API_KEY\"] = userdata.get('WITHPI_API_KEY')\n", "\n", "client = PiClient()"]}, {"cell_type": "markdown", "metadata": {"id": "s7RRO3iXjYbY"}, "source": ["# Load contract and Dataset\n", "\n", "Load the `Aesop AI` example and example set from Pi Labs cookbooks, or edit below to load a different one.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "oXJmb89i5iN5"}, "outputs": [], "source": ["import httpx\n", "import pandas as pd\n", "from google.colab import data_table\n", "from withpi.types import Contract\n", "\n", "resp = httpx.get(\"https://raw.githubusercontent.com/withpi/cookbook-withpi/refs/heads/main/contracts/aesop_ai.json\")\n", "\n", "aesop_contract = Contract.model_validate_json(resp.content)\n", "\n", "for dimension in aesop_contract.dimensions:\n", "  print(dimension.label)\n", "  for sub_dimension in dimension.sub_dimensions:\n", "    print(f\"\\t{sub_dimension.description}\")\n", "\n", "df = pd.read_parquet(\"https://raw.githubusercontent.com/withpi/cookbook-withpi/refs/heads/main/datasets/aesop_ai_examples.parquet\")\n", "data_table.enable_dataframe_formatter()\n", "df\n"]}, {"cell_type": "markdown", "metadata": {"id": "j1FAoBqU7dwf"}, "source": ["## Optimize your prompt\n", "\n", "Kick off a prompt optimization run.  This will operate in the background."]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "4NpCZhP6exgI"}, "outputs": [], "source": ["prompt_optimization_status = client.prompt.optimize(\n", "    contract=aesop_contract,\n", "    dspy_optimization_type=\"COPRO\",\n", "    examples=[{\"llm_input\": row[\"input\"], \"llm_output\": row[\"output\"]} for index, row in df.iterrows()],\n", "    initial_system_instruction=aesop_contract.description,\n", "    model_id=\"gpt-4o-mini\",\n", "    tuning_algorithm=\"DSPY\",\n", ")"]}, {"cell_type": "markdown", "metadata": {"id": "FYT2a6-7zHVf"}, "source": ["## Check for completion\n", "\n", "The following cell will connect to the tail of logs while optimization proceeds.  It will take order of **10 minutes**"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "01LXvjOMte7D"}, "outputs": [], "source": ["import json\n", "\n", "while True:\n", "  optimized_response = client.prompt.get_status(job_id=prompt_optimization_status.job_id)\n", "  if (optimized_response.state != 'QUEUED') and (optimized_response.state != 'RUNNING'):\n", "    break\n", "\n", "  with client.prompt.with_streaming_response.stream_messages(\n", "      job_id=prompt_optimization_status.job_id, timeout=None) as response:\n", "    for line in response.iter_lines():\n", "          print(line)\n", "\n", "optimized = json.dumps(optimized_response.optimized_prompt_messages, indent=2)\n", "display(optimized)"]}, {"cell_type": "markdown", "metadata": {"id": "rMTmieU9xPgT"}, "source": ["## Save the new system prompt template\n", "\n", "It's convenient to stash this template for use later."]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "tpWpFWfEwo4j"}, "outputs": [], "source": ["import json\n", "from pathlib import Path\n", "from google.colab import files\n", "\n", "filename = 'aesop_ai_dspy_prompt.json.jinja'\n", "Path(filename).write_text(optimized)\n", "files.download(filename)"]}, {"cell_type": "markdown", "metadata": {"id": "GXdTY-z8zpn7"}, "source": ["#Run inference with updated prompt\n", "\n", "The underlying library is https://dspy.ai/, which emits a prompt in templated form.  It's a relatively straightforward Jinja2 template."]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "WZk4WJN80Ikc"}, "outputs": [], "source": ["from jinja2 import Template\n", "from concurrent.futures import ThreadPoolExecutor\n", "from tqdm.notebook import tqdm\n", "from litellm import completion\n", "import os\n", "import re\n", "from google.colab import userdata\n", "\n", "os.environ[\"GEMINI_API_KEY\"] = userdata.get('GOOGLE_API_KEY')\n", "\n", "prompt_template = Template(optimized)\n", "result_extractor = re.compile(r\".*\\[\\[ ## response ## \\]\\](.*)\\[\\[ ## completed ## \\]\\]\", re.DOTALL)\n", "\n", "def generate(prompt: str, pbar) -> str:\n", "  messages = json.loads(prompt_template.render(input=prompt))\n", "  result = completion(model=\"gemini/gemini-1.5-flash-8b-latest\",\n", "                      messages=messages).choices[0].message.content\n", "\n", "  pbar.update(1)\n", "  return result_extractor.match(result).group(1)\n", "\n", "def do_inference():\n", "  futures = []\n", "  pbar = tqdm(total=len(df))\n", "  with ThreadPoolExecutor(max_workers=4) as executor:\n", "    for index, row in df.iterrows():\n", "      futures.append(executor.submit(generate, row[\"input\"], pbar))\n", "  return [future.result() for future in futures]\n", "\n", "df[\"output\"] = do_inference()\n", "df"]}, {"cell_type": "markdown", "metadata": {"id": "urJa5GG1A1EQ"}, "source": ["# Save the updated dataset\n", "\n", "Feel free to compare results from before with results after prompt optimization.  Save the updated dataset for later"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "RwtVKOB6A_Og"}, "outputs": [], "source": ["from google.colab import files\n", "\n", "filename = \"aesop_ai_examples_optimized.parquet\"\n", "df.to_parquet(filename)\n", "files.download(filename)"]}, {"cell_type": "markdown", "metadata": {"id": "jT7s_nuJsHbM"}, "source": ["## Next Steps\n", "\n", "Now that you have an improved prompt using an **uncalibrated** contract, let's try **calibrating** the contract!  This should help it more closely align with what you actually value.  Proceed on to the [Contract Calibration](https://colab.research.google.com/github/withpi/cookbook-withpi/blob/main/colabs/Contract_Calibration.ipynb) colab to do this."]}], "metadata": {"colab": {"include_colab_link": true, "provenance": []}, "kernelspec": {"display_name": "Python 3", "name": "python3"}, "language_info": {"name": "python"}}, "nbformat": 4, "nbformat_minor": 0}