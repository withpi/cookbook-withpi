{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/withpi/cookbook-withpi/blob/main/colabs/DSPy_Optimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://withpi.ai\"><img src=\"https://withpi.ai/logoFullBlack.svg\" width=\"240\"></a>\n",
        "\n",
        "<a href=\"https://code.withpi.ai\"><font size=\"4\">Documentation</font></a>\n",
        "\n",
        "<a href=\"https://play.withpi.ai\"><font size=\"4\">Technique Catalog</font></a>"
      ],
      "metadata": {
        "id": "pi-masthead"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bwm4tjdnedp6"
      },
      "source": [
        "# Prompt Optimization Technique\n",
        "\n",
        "\n",
        "This colab assumes that you already went through [Input Generation](https://colab.research.google.com/github/withpi/cookbook-withpi/blob/main/colabs/Input_Generation.ipynb), and now wish to optimize your system prompt\n",
        "\n",
        "We will walk through the same `Aesop AI` example, but you can load any contract here. Let's dig in!\n",
        "\n",
        "This should take about **15 minutes**, even if you're unfamiliar with Colab."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install and initialize SDK\n",
        "\n",
        "Connect to a regular CPU Python 3 runtime.  You won't need GPUs for this notebook.\n",
        "\n",
        "You'll need a WITHPI_API_KEY from https://play.withpi.ai.  Add it to your notebook secrets (the key symbol) on the left.\n",
        "\n",
        "Run the cell below to install packages and load the SDK"
      ],
      "metadata": {
        "id": "pi-setup-markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pi-setup"
      },
      "outputs": [],
      "execution_count": null,
      "source": [
        "%%capture\n",
        "\n",
        "%pip install withpi litellm\n",
        "\n",
        "import os\n",
        "from google.colab import userdata\n",
        "from litellm import completion\n",
        "from withpi import PiClient\n",
        "\n",
        "os.environ[\"WITHPI_API_KEY\"] = userdata.get('WITHPI_API_KEY')\n",
        "\n",
        "client = PiClient()\n",
        "\n",
        "def print_contract(contract):\n",
        "  for dimension in contract.dimensions:\n    print(dimension.label)\n",
        "  for sub_dimension in dimension.sub_dimensions:\n",
        "    print(f\"\\t{sub_dimension.description}\")\n",
        "\n",
        "def generate(system: str, user: str, model: str) -> str:\n",
        "  messages = [\n",
        "    {\n",
        "      \"content\": system,\n",
        "      \"role\": \"system\"\n",
        "    },\n",
        "    {\n",
        "      \"content\": prompt,\n",
        "      \"role\": \"user\"\n",
        "    }\n",
        "  ]\n",
        "  return completion(model=model,\n",
        "                    messages=messages).choices[0].message.content\n",
        "\n",
        "class printer(str):\n",
        "  def __repr__(self):\n",
        "    return self\n",
        "def prettyprint(response: str):\n",
        "  display(printer(response))\n",
        "\n",
        "def print_scores(pi_scores):\n",
        "  for dimension_name, dimension_scores in pi_scores.dimension_scores.items():\n",
        "    print(f\"{dimension_name}: {dimension_scores.total_score}\")\n",
        "    for subdimension_name, subdimension_score in dimension_scores.subdimension_scores.items():\n",
        "      print(f\"\\t{subdimension_name}: {subdimension_score}\")\n",
        "    print(\"\\n\")\n",
        "  print(\"---------------------\")\n",
        "  print(f\"Total score: {pi_scores.total_score}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7RRO3iXjYbY"
      },
      "source": [
        "# Load contract and Dataset\n",
        "\n",
        "Load the `Aesop AI` example and example set from Pi Labs cookbooks, or edit below to load a different one.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oXJmb89i5iN5"
      },
      "outputs": [],
      "source": [
        "import httpx\n",
        "import pandas as pd\n",
        "from google.colab import data_table\n",
        "from withpi.types import Contract\n",
        "\n",
        "resp = httpx.get(\"https://raw.githubusercontent.com/withpi/cookbook-withpi/refs/heads/main/contracts/aesop_ai.json\")\n",
        "\n",
        "aesop_contract = Contract.model_validate_json(resp.content)\n",
        "\n",
        "for dimension in aesop_contract.dimensions:\n",
        "  print(dimension.label)\n",
        "  for sub_dimension in dimension.sub_dimensions:\n",
        "    print(f\"\\t{sub_dimension.description}\")\n",
        "\n",
        "df = pd.read_parquet(\"https://raw.githubusercontent.com/withpi/cookbook-withpi/refs/heads/main/datasets/aesop_ai_examples.parquet\")\n",
        "data_table.enable_dataframe_formatter()\n",
        "df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1FAoBqU7dwf"
      },
      "source": [
        "## Optimize your prompt\n",
        "\n",
        "Kick off a prompt optimization run.  This will operate in the background."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4NpCZhP6exgI"
      },
      "outputs": [],
      "source": [
        "prompt_optimization_status = client.prompt.optimize(\n",
        "    contract=aesop_contract,\n",
        "    dspy_optimization_type=\"COPRO\",\n",
        "    examples=[{\"llm_input\": row[\"input\"], \"llm_output\": row[\"output\"]} for index, row in df.iterrows()],\n",
        "    initial_system_instruction=aesop_contract.description,\n",
        "    model_id=\"gpt-4o-mini\",\n",
        "    tuning_algorithm=\"DSPY\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYT2a6-7zHVf"
      },
      "source": [
        "## Check for completion\n",
        "\n",
        "The following cell will connect to the tail of logs while optimization proceeds.  It will take order of **10 minutes**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01LXvjOMte7D"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "while True:\n",
        "  optimized_response = client.prompt.get_status(job_id=prompt_optimization_status.job_id)\n",
        "  if (optimized_response.state != 'QUEUED') and (optimized_response.state != 'RUNNING'):\n",
        "    break\n",
        "\n",
        "  with client.prompt.with_streaming_response.stream_messages(\n",
        "      job_id=prompt_optimization_status.job_id, timeout=None) as response:\n",
        "    for line in response.iter_lines():\n",
        "          print(line)\n",
        "\n",
        "optimized = json.dumps(optimized_response.optimized_prompt_messages, indent=2)\n",
        "display(optimized)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMTmieU9xPgT"
      },
      "source": [
        "## Save the new system prompt template\n",
        "\n",
        "It's convenient to stash this template for use later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tpWpFWfEwo4j"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "from google.colab import files\n",
        "\n",
        "filename = 'aesop_ai_dspy_prompt.json.jinja'\n",
        "Path(filename).write_text(optimized)\n",
        "files.download(filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXdTY-z8zpn7"
      },
      "source": [
        "#Run inference with updated prompt\n",
        "\n",
        "The underlying library is https://dspy.ai/, which emits a prompt in templated form.  It's a relatively straightforward Jinja2 template."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZk4WJN80Ikc"
      },
      "outputs": [],
      "source": [
        "from jinja2 import Template\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from tqdm.notebook import tqdm\n",
        "from litellm import completion\n",
        "import os\n",
        "import re\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"GEMINI_API_KEY\"] = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "prompt_template = Template(optimized)\n",
        "result_extractor = re.compile(r\".*\\[\\[ ## response ## \\]\\](.*)\\[\\[ ## completed ## \\]\\]\", re.DOTALL)\n",
        "\n",
        "def generate(prompt: str, pbar) -> str:\n",
        "  messages = json.loads(prompt_template.render(input=prompt))\n",
        "  result = completion(model=\"gemini/gemini-1.5-flash-8b-latest\",\n",
        "                      messages=messages).choices[0].message.content\n",
        "\n",
        "  pbar.update(1)\n",
        "  return result_extractor.match(result).group(1)\n",
        "\n",
        "def do_inference():\n",
        "  futures = []\n",
        "  pbar = tqdm(total=len(df))\n",
        "  with ThreadPoolExecutor(max_workers=4) as executor:\n",
        "    for index, row in df.iterrows():\n",
        "      futures.append(executor.submit(generate, row[\"input\"], pbar))\n",
        "  return [future.result() for future in futures]\n",
        "\n",
        "df[\"output\"] = do_inference()\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urJa5GG1A1EQ"
      },
      "source": [
        "# Save the updated dataset\n",
        "\n",
        "Feel free to compare results from before with results after prompt optimization.  Save the updated dataset for later"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RwtVKOB6A_Og"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "filename = \"aesop_ai_examples_optimized.parquet\"\n",
        "df.to_parquet(filename)\n",
        "files.download(filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jT7s_nuJsHbM"
      },
      "source": [
        "## Next Steps\n",
        "\n",
        "Now that you have an improved prompt using an **uncalibrated** contract, let's try **calibrating** the contract!  This should help it more closely align with what you actually value.  Proceed on to the [Contract Calibration](https://colab.research.google.com/github/withpi/cookbook-withpi/blob/main/colabs/Contract_Calibration.ipynb) colab to do this."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}